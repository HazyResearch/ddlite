{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"imgs/logo.jpg\" width=\"50px\" style=\"margin-right:10px\">\n",
    "\n",
    "# Snorkel Workshop: Extracting Spouse Relations <br> from the News\n",
    "## Part 4: Training our End Extraction Model\n",
    "\n",
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last tutorial part to train our end machine learning model.\n",
    "\n",
    "For this tutorial, we will be training a fairly effective deep learning model. More generally, however, Snorkel plugs in with many ML libraries, making it easy to use almost any state-of-the-art model as the end model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from snorkel.model.utils import MetalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading Candidates and Gold Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('fast_dev_data.pkl', 'rb') as f:\n",
    "    dev_data = pickle.load(f)\n",
    "    dev_labels = pickle.load(f)\n",
    "    \n",
    "with open('fast_train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    train_labels = pickle.load(f)\n",
    "    \n",
    "with open('fast_test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    test_labels = pickle.load(f)\n",
    "\n",
    "with open('train_marginals.pkl', 'rb') as f:\n",
    "    train_marginals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Training a _Long Short-term Memory_ (LSTM) Neural Network\n",
    "\n",
    "[LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) can acheive state-of-the-art performance on many text classification tasks. We'll train a simple LSTM model below. \n",
    "\n",
    "In deep learning, hyperparameter tuning is very important and computationally expensive step in training models. For purposes of this tutorial, we have a pre-trained model using the training labels generated from the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare the input to our LSTM by adding *markers* to the beginning and end of the person mentions so the LSTM knows which two persons in the sentence we want to learn a relation for. We then featurize the tokens it using a standard vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EmbeddingFeaturizer\n",
    "from utils import mark_entities\n",
    "\n",
    "markers = ['[[BEGIN0]]','[[END0]]','[[BEGIN1]]','[[END1]]']\n",
    "featurizer = EmbeddingFeaturizer(markers=markers)\n",
    "\n",
    "def convert_to_lstm_input(data):\n",
    "    X = []\n",
    "    #mark candidates with markers\n",
    "    for i in range(len(data)):\n",
    "        cand = data.loc[i]\n",
    "        marked_tokens = mark_entities(\n",
    "                    cand.tokens,\n",
    "                    positions=[cand.person1_word_idx, cand.person2_word_idx],\n",
    "                    markers=markers)\n",
    "        X.append(marked_tokens)\n",
    "        \n",
    "    #featurize string tokens tokens\n",
    "    featurizer.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "train_X_tensor = convert_to_lstm_input(train_data)\n",
    "dev_X_tensor = convert_to_lstm_input(dev_data)\n",
    "test_X_tensor = convert_to_lstm_input(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../snorkel/mtl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "datasets.append(MetalDataset(train_X_tensor, torch.LongTensor(train_marginals[:,0]))) #TODO: check \n",
    "datasets.append(MetalDataset(dev_X_tensor, torch.LongTensor(dev_labels+1.)))\n",
    "datasets.append(MetalDataset(test_X_tensor, torch.LongTensor(test_labels+1.)))\n",
    "\n",
    "dataloaders = []\n",
    "for dataset, split in zip(datasets, [\"train\", \"valid\", \"test\"]):\n",
    "    dataloader = DataLoader(dataset)\n",
    "    dataloader.split = split\n",
    "    dataloaders.append(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snorkel.mtl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-46ba657ce7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../snorkel/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultitaskDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultitaskDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupgrade_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snorkel.mtl'"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from snorkel.mtl.data import MultitaskDataset, MultitaskDataLoader\n",
    "\n",
    "def upgrade_dataloaders(dataloaders: List[DataLoader]):\n",
    "    new_dataloaders = []\n",
    "    for dataloader in dataloaders:\n",
    "        dataset = dataloader.dataset\n",
    "\n",
    "        new_dataset = MultitaskDataset(\n",
    "            name=f\"data_{dataloader.split}\", \n",
    "            X_dict={\"data\": dataset.X},  # This op is specific to TensorDataset\n",
    "            Y_dict={\"labels\": dataset.Y} # Maybe\n",
    "        )\n",
    "        new_dataloader = MultitaskDataLoader(\n",
    "            task_to_label_dict={\"task\": \"labels\"},\n",
    "            dataset=new_dataset,\n",
    "            split=dataloader.split,\n",
    "            batch_size=dataloader.batch_size,\n",
    "            shuffle=(dataloader.split == \"train\")\n",
    "        )\n",
    "        new_dataloaders.append(new_dataloader)\n",
    "    return new_dataloaders\n",
    "\n",
    "dataloaders = upgrade_dataloaders(dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM Model\n",
    "For purposes of this tutorial, we have saved a pre-trained model that was trained using probabilistic labels generated in the previous notebook. \n",
    "\n",
    "We define our model here and load the pretrained weights before evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from snorkel.mtl.simple_model import SimpleModel\n",
    "from utils import LSTMModule, EmbeddingsEncoder\n",
    "\n",
    "MAX_INT = train_X_tensor.max()\n",
    "embed_size = 4\n",
    "hidden_size = 5\n",
    "\n",
    "lstm_module = LSTMModule(\n",
    "    embed_size,\n",
    "    hidden_size,\n",
    "    bidirectional=False,\n",
    "    verbose=False,\n",
    "    lstm_reduction=\"attention\",\n",
    "    encoder_class=EmbeddingsEncoder,\n",
    "    encoder_kwargs={\"vocab_size\": MAX_INT + 1},\n",
    ")\n",
    "\n",
    "model = SimpleModel(\n",
    "    modules=[\n",
    "    lstm_module,\n",
    "    nn.Linear(lstm_module.output_dim,1)],\n",
    "    metrics = ['accuracy', 'f1', 'precision','recall'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:: 100%|██████████| 22254/22254 [04:37<00:00, 80.33it/s]\n",
      "Epoch 1:: 100%|██████████| 22254/22254 [04:43<00:00, 78.48it/s]\n",
      "Epoch 2:: 100%|██████████| 22254/22254 [05:00<00:00, 74.09it/s]\n",
      "Epoch 3:: 100%|██████████| 22254/22254 [04:39<00:00, 79.58it/s]\n",
      "Epoch 4:: 100%|██████████| 22254/22254 [04:48<00:00, 77.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train SimpleModel\n",
    "from snorkel.mtl.trainer import Trainer\n",
    "trainer = Trainer(progress_bar=True, n_epochs=5)\n",
    "trainer.train_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get the precision, recall, and F1 score from the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task/data_valid/valid/accuracy': 0.3624161073825503, 'task/data_valid/valid/f1': 0.5320197044334976, 'task/data_valid/valid/precision': 0.3624161073825503, 'task/data_valid/valid/recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "scores = model.score(dataloaders[1:2])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task/data_test/test/accuracy': 0.28289473684210525, 'task/data_test/test/f1': 0.441025641025641, 'task/data_test/test/precision': 0.28289473684210525, 'task/data_test/test/recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "scores = model.score(dataloaders[2])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./trained_spouse_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = SimpleModel(\n",
    "#     modules=[\n",
    "#     lstm_module,\n",
    "#     nn.Linear(lstm_module.output_dim,1)],\n",
    "#     metrics = ['accuracy', 'f1', 'precision','recall'])\n",
    "# loaded_model.load('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = loaded_model.score(dataloaders[2])\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dev_data)):\n",
    "    cand = dev_data.loc[i]\n",
    "    cand['tokens'] = [str(word.lstrip().strip(\"'\")) for word in cand['tokens'] if isinstance(word, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    cand = train_data.loc[i]\n",
    "    cand['tokens'] = [str(word.lstrip().strip(\"'\")) for word in cand['tokens'] if isinstance(word, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    cand = test_data.loc[i]\n",
    "    cand['tokens'] = [str(word.lstrip().strip(\"'\")) for word in cand['tokens'] if isinstance(word, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fast_dev_data.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_data, f)\n",
    "    pickle.dump(dev_labels,f)\n",
    "    \n",
    "with open('fast_train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "    pickle.dump(train_labels,f)\n",
    "    \n",
    "with open('fast_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "    pickle.dump(test_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
