<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Snorkel</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    
  </head>
  <body>
    <div class="wrapper">
      <header>
        <!--<h1>Snorkel</h1>-->
        <img src="figs/logo_01.png" width="100"/>
        <p>
          Building and Managing Training Data
        </p>

        <p class="view">
          <a href="https://github.com/HazyResearch/snorkel">View the Project on GitHub
        </p>

        <a class="github-button" href="https://github.com/HazyResearch/snorkel" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star HazyResearch/snorkel on GitHub">Star</a>

        <a class="github-button" href="https://github.com/HazyResearch/snorkel/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork HazyResearch/snorkel on GitHub">Fork</a>

        <p></p>
        <h2>
            <a id="motivation" class="anchor" href="#motivation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

            <p>[6/4/19] Two papers using Snorkel (<a href="https://nature-research-under-consideration.nature.com/users/37265-nature-communications/posts/38921-weakly-supervised-classification-of-rare-aortic-valve-malformations-using-unlabeled-cardiac-mri-sequences">cardiac MRI imaging</a>, <a href="https://ai.stanford.edu/~kuleshov/papers/gwaskb-manuscript.pdf">GWAS studies</a>) accepted to Nature Communications</p> 
            <p></p> 

            <p>[4/21/19] Two papers around <a href="https://arxiv.org/abs/1903.05844">learning weak supervision structure</a> and <a href="https://arxiv.org/abs/1803.06084">augmentation theory</a> accepted to ICML 2019</p>
            <p></p> 

            <p>[3/14/19] <a href="https://arxiv.org/abs/1812.00417">SIGMOD paper</a> and <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google AI blog</a> on Snorkel's usage at Google</p>  


            <!-- MAX 3 ITEMS -->
            <a onclick="news()">More... </a>
            <p></p>
            <div id="news"style="display:none">
                <p>[3/29/19] <a href="https://nature-research-under-consideration.nature.com/users/37265-nature-communications/posts/47370-cross-modal-data-programming-enables-rapid-medical-machine-learning">Manuscript</a>  on applying Snorkel to radiology and neurology applications posted</p>
                <p></p> 

                <p>[3/21/19] Snorkel achieves <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">SOTA on GLUE Benchmark</a></p> 
                <p></p> 

                <p>[12/05/18] Snorkel shout-out in Kunle Olukotun's <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=12469">keynote at NeurIPS 2018</a> </p>
                <p></p>

            </div>

      </header>
      <div>
          <p></p>
          <p></p>
        <section>


<h2>Snorkel: Building and Managing Training Data</h2>
<p>
  Programming abstractions in machine learning are changing: practitioners are spending less time on architectures and hardware optimizations and, instead, focusing on training data. We find that spending time <em><font color="6699CC">programmatically building and manipulating the training data</font></em> provides a powerful and effective strategy to achieve high performance in ML pipelines. Snorkel focuses on three key abstractions for building and managing training datasets: <em><font color="6699CC">labeling data, transforming data, and slicing data.</font></em> 
</p>
<img align:center src="figs/fig_abstractions.png" alt="Snorkel" width=90% />

<p></p>
<hr>
<h2>Snorkel in the Wild!</h2>

<img align:right src="figs/logos.png" width="90%"  />
<p></p>

<h3>Snorkel in Industry</h3>
<p>
    Snorkel has been used from <b><font color="6699CC"><a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">industrial scale at Google</a></font></b> in Snorkel Drybell, <b><font color="6699CC"><a href="https://arxiv.org/pdf/1812.06176.pdf">IBM bootstrapping conversational agents</a></font></b>,  <b><font color="6699CC"><a href="https://www.edsurge.com/news/2018-03-01-cheating-on-chegg-maybe-not-on-its-tutoring-platform">Chegg catching cheating</a></font></b>, and  <b><font color="6699CC"><a href="https://ajratner.github.io/assets/papers/Osprey_DEEM.pdf">Intel extracting rare relations</a></font></b>. 
</p>
<p>
    Snorkel has also achieved state-of-the-art performance on the <b><font color="6699CC"><a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">GLUE</a></font></b> and
    <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">SuperGLUE language understanding benchmark</a></font></b>! 
</p>
<p></p>

<h3>Snorkel in Science</h3>
<p>
    Snorkel has been used extensively in medical settings, with two <em><font color="6699CC">Nature Communication</font></em> publications around <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">automated GWAS curation</a></font></b> and <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">cardiac MRI classification</a></font></b>. 
    Snorkel can also handle <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">cross-modal</a></font></b> classification to label radiographs and EEG signals based on labels applied to associated text reports. Recently, Snorkel has been used to label electronic health record (EHR) data, offering a scalable solution for national <b><font color="6699CC"><a href="https://arxiv.org/abs/1904.07640">medical device surveillance</a></font></b>.
</p>
<p></p>
<img align:center src="figs/nature.png" alt="Snorkel" width=100% />

<p></p>
<p>
    Other adaptations and use cases of Snorkel can be found under <b><font color="6699CC">Resources</font></b>. 
</p>
<!-- <p>
    A selection of our other use cases include  <b><font color="6699CC">IBM bootstrapping conversational agents</font></b>,  <b><font color="6699CC">Chegg catching cheating</font></b>, and  <b><font color="6699CC">Intel extracting rare relations</font></b>. Other adaptations of Snorkel can be found in our References section. 
</p> -->

<p></p>
<hr>
<h2>What does Snorkel do?</h2>

<p>
    Snorkel is a general framework that encapsulates several <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">weak supervision techniques</a></font></b> and allows domain experts to encode their knowledge programmatically to provide supervision through the following techniques:
</p>
 
<h3>
    <a id="users" class="anchor" href="#users" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Labeling
</h3>

<img align:center src="figs/lfs.png" alt="Snorkel" width=100% />
<p></p>

<p>
    Snorkel takes as input labeling functions (LFs) to heuristically label training examples. It denoises these LFs by learning their accuracies and correlation structure without using any hand-labeled data. Read more here:
    <ul>
        <li>
            <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">Blog on using Labeling Functions to Efficiently Label Training Data</a>
        </li>

        <li>
            <a href="https://arxiv.org/abs/1605.07723">NeurIPS'16 Paper on Data Programming</a> 
        </li>

        <li>
            <a href="https://arxiv.org/abs/1711.10160">VLDB'18 Paper on System for Labeling with LFs</a>
        </li>

        <li>
            <a href="https://arxiv.org/abs/1810.02840">AAAI'19 Paper on Core Method and Supporting Theory</a>
        </li>
      </ul>
</p>


<h3>
    <a id="users" class="anchor" href="#users" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tranforming
</h3>
<img align:center src="figs/tfs.png" alt="Snorkel" width=100% />
<p></p>

<p>
    Snorkel takes as input transformation functions (TFs) to heuristically generate new, modified training examples. It learns compositions of transformations across various domain-specific tasks to optimize for a representative training set. Read more here:
    <ul>
        <li>
          <a href="https://hazyresearch.github.io/snorkel/blog/tanda.html">Blog on using Transformation Functions to Augment Training Set</a>
        </li>

        <li>
          <a href="https://arxiv.org/abs/1709.01643">NeurIPS'17 Paper on Core Method and Supporting Theory</a>
        </li>

        <li>
            <a href="https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html">Follow Up: Google's AutoAugment to Learn Augmentation Policies</a>
          </li>
      </ul>
</p>

<h3>Slicing</h3>

<p>
    Snorkel takes as input slicing functions (SFs) to heuristically identify subsets of the data the model should particularly care about. It models slices in the style of multi-task learning and an attention-mechanism is then learned over these heads. Read more here:
    <ul>
        <li>
            <a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">Blog on using Slicing Functions to achieves SOTA Performance</a>
        </li>
      </ul>
</p>

<p>
    Snorkel can also operate over other forms of weak supervision like crowdsourcing by modeling individual workers as labeling functions. To properly take advantage of all supervision signal available, Snorkel can takes advantage of multi-task learning and transfer learning, moving towards <b><font color="6699CC"><a href="https://ajratner.github.io/assets/papers/software_2_mmt_vision.pdf">massive multi-task learning</a></font></b> to facilitate incorporating diverse and varying granularities of supervision at a large scale. 
</p>

<hr>
<a href=#resources><h2>Additional Resources</h2></a>


<h3><a onclick="blogs()">Blogs and Tutorials </a></h3>
<div id="blogs">
  <ul>
      <li>
        [3/23/2019] <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">Massive Multi-Task Learning with Snorkel MeTaL: Bringing More Supervision to Bear</a>
      </li>
      <li>
        [2/4/2019] <a href="https://hazyresearch.github.io/snorkel/blog/mtl_systems.html">Emerging Topics in Multi-Task Learning Systems</a>
      </li>
      <li>
        [12/4/2018] <a href="http://hazyresearch.github.io/snorkel/blog/s2_programming.html">Software 2.0 and the Paradigm Shift in Programming ML Systems</a>
      </li>
      <li>
        [06/21/2018] <a href="http://dawn.cs.stanford.edu/2018/06/21/debugging/">Systematically Debugging Training Data for Software 2.0</a>
      </li>
      <li>
        [11/30/2017] <a href="https://hazyresearch.github.io/snorkel/blog/snorkel_programming_training_data.html">Weak Supervision: The New Programming Language for Software 2.0</a>
      </li>
      <li>
        [09/20/2017] <a href="http://dawn.cs.stanford.edu/2017/09/14/coral/">Exploiting Building Blocks of Data to Efficiently Create Training Sets</a>
      </li>
      <li>
        [08/10/2017] <a href="https://hazyresearch.github.io/snorkel/blog/tanda.html">Learning to Compose Domain-Specific Transformations for Data Augmentation</a> [<a href="https://github.com/HazyResearch/tanda">Repo</a>]
      </li>
      <li>
        [07/12/2017] <a href="https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html">Weak Supervision: The New Programming Paradigm for Machine Learning</a>
      </li>
      <li>
        [06/05/2017] <a href="https://hazyresearch.github.io/snorkel/blog/snark.html">Scaling Up Snorkel with Spark</a> [<a href="https://github.com/HazyResearch/snorkel/blob/master/tutorials/snark/Snark%20Tutorial.ipynb">Tutorial</a>]
      </li>
      <li>
        [05/08/2017] <a href="https://hazyresearch.github.io/snorkel/blog/holoclean.html">HoloClean: Weakly Supervised Data Repairing</a>
      </li>
      <li>
        [04/17/2017] <a href="https://hazyresearch.github.io/snorkel/blog/structure_learning.html">Structure Learning: Are Your Sources Only Telling You What You Want to Hear?</a> [<a href="https://github.com/HazyResearch/snorkel/blob/master/tutorials/advanced/Structure_Learning.ipynb">Tutorial</a>]
      </li>
      <li>
        [03/21/2017] <a href="https://hazyresearch.github.io/snorkel/blog/babble_labble.html">Babble Labble: Learning from Natural Language Explanations</a>
      </li>
      <li>
        [03/16/2017] <a href="https://hazyresearch.github.io/snorkel/blog/fonduer.html">Fonduer: Knowledge Base Construction from Richly Formatted Data</a>
      </li>
      <li>
        [12/15/2016] <a href="http://hazyresearch.github.io/snorkel/blog/dp_with_tf_blog_post.html">Data Programming + TensorFlow Tutorial</a> (<a href="http://hazyresearch.github.io/snorkel/blog/DataProgrammingTensorFlow-v2.ipynb">notebook version</a>)
      </li>
      <li>
        [11/24/2016] <a href="http://hazyresearch.github.io/snorkel/blog/slimfast.html">SLiMFast: Assessing the Reliability of Data</a>
      </li>
      <li>
        [10/24/2016] <a href="http://hazyresearch.github.io/snorkel/blog/socratic_learning.html">Socratic Learning: Debugging ML Models</a>
      </li>
      <li>
        [9/19/2016] <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">Data Programming: ML with Weak Supervision</a> [<a href="https://github.com/HazyResearch/snorkel/tree/master/tutorials/intro">Tutorial</a>]
      </li>
      </ul>
</div>

<p></p>
<h3><a onclick="papers()">Papers and Pre-Prints</a></h3>
<div id="papers">
    <ul>
        <li>
          <b><a href="https://arxiv.org/abs/1711.10160">Snorkel: Rapid Training Data Creation with Weak Supervision</a> (VLDB 2018)</b>
        </li>
      <li>
        <b><a href="https://arxiv.org/abs/1605.07723">Data Programming: Creating Large Training Sets, Quickly</a> (NeurIPS 2016)</b>
      </li>
      <li>
          <b><a href="https://cs.stanford.edu/~chrismre/papers/Chris_Re-KDD.pdf">Snorkel and the Software 2.0 vision</a> (KDD 2018)</b>
      </li>
        <b><a href="https://arxiv.org/abs/1703.00854">Learning the Structure of Generative Models without Labeled Data</a> (ICML 2017)</b>
      </li>
      <li>
        <b><a href="https://arxiv.org/pdf/1903.05844.pdf">Learning Dependency Structures for Weak Supervision Models</a> (Arxiv 2019)</b>
      </li>
      <li>
        <b><a href="https://arxiv.org/abs/1810.02840">Training Complex Models with Multi-Task Weak Supervision</a> (AAAI 2019)</b>
      </li>
      <li>
        <b><a href="https://ajratner.github.io/assets/papers/software_2_mmt_vision.pdf">The Role of Massively Multi-Task and Weak Supervision in Software 2.0</a> (CIDR 2019)</b>
      </li>
      <li>
        <a href="pdfs/snorkel_demo.pdf">Snorkel: Fast Training Set Generation for Information Extraction</a> (SIGMOD DEMO 2017)
      </li>
      <li>
        <a href="https://arxiv.org/abs/1709.02477">Inferring Generative Model Structure with Static Analysis</a> (NeurIPS 2017)
      </li>
      <li>
        <a href="https://arxiv.org/abs/1805.03818">Training Classifiers with Natural Language Explanations</a> (ACL 2018)
      </li>
      <li>
        <a href="http://cs.stanford.edu/people/chrismre/papers/DDL_HILDA_2016.pdf">Data Programming with DDLite: Putting Humans in a Different Part of the Loop</a> (HILDA @ SIGMOD 2016; note Snorkel was previously <em>DDLite</em>)
      </li>
      <li>
        <a href="https://arxiv.org/abs/1610.08123">Socratic Learning: Correcting Misspecified Generative Models using Discriminative Models</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/1703.05028" target="_blank">Fonduer: Knowledge Base Construction from Richly Formatted Data</a> (SIGMOD 2018)
      </li>
      <li>
          <a href="https://arxiv.org/abs/1709.01643">Learning to Compose Domain-Specific Transformations for Data Augmentation</a> (NeurIPS 2017)
        </li>
        <li>
          <a href="https://arxiv.org/abs/1709.02605">Gaussian Quadrature for Kernel Features</a> (NeurIPS 2017)
        </li>
      </ul>
</div>

<p></p>
<h3><a onclick="uses()">Snorkel Use Cases</a></h3>
<div id="uses">
  <ul>
<li>
    Conversational agents at IBM: <a href="https://arxiv.org/pdf/1812.06176.pdf">Bootstrapping Conversational Agents With Weak Supervision (AAAI 2019)</a>
  </li>
  <li>
    Web content & event classification at Google: <a href="https://arxiv.org/abs/1812.00417">Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale (SIGMOD Industry 2019)</a>, and <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google AI blog post</a>
  </li>
  <li>
    Business intelligence at Intel: <a href="https://ajratner.github.io/assets/papers/Osprey_DEEM.pdf">Osprey: Non-Programmer Weak Supervision of Imbalanced Extraction Problems (SIGMOD DEEM 2019)</a>
  </li>
  <li>
    Anti-semitic tweet classification w/ Snorkel + transfer learning: <a href="https://t.co/h0zGQwDD59">A Technique for Building NLP Classifiers Efficiently with Transfer Learning and Weak Supervision (Blog post 2019)</a>
  </li>
  <li>
      Clinical text classification: <a href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-018-0723-6">A clinical text classification paradigm using weak supervision and deep representation (BMC MIDM 2019)</a>
    </li>
  <li>
    Social media text mining: <a href="https://ieeexplore.ieee.org/abstract/document/8609589/authors#authors">Deep Text Mining of Instagram Data without Strong Supervision (ICWI 2018)</a>
  </li>
  <li>
    Cardiac MRI classification with Stanford Medicine: <a href="https://www.biorxiv.org/content/biorxiv/early/2018/06/05/339630.full.pdf">Weakly supervised classification of rare aortic valve malformations using unlabeled cardiac MRI sequences (BioArxiv 2018)</a>
  </li>
  <li>
    Catching cheating at Chegg (<a href="https://www.edsurge.com/news/2018-03-01-cheating-on-chegg-maybe-not-on-its-tutoring-platform">Article</a>)
  </li>
  <li>
    Medical image triaging at Stanford Radiology: <a href="#">Cross-Modal Data Programming for Medical Images (NeurIPS ML4H 2017)</a>
  </li>
  <li>
    GWAS KBC with Stanford Genomics: <a href="https://ajratner.github.io/assets/papers/neurips2016-healthcare.pdf">A Machine-Compiled Database of Genome-Wide Association Studies (NeurIPS ML4H 2016)</a>
  </li>
</ul>

</section>
<!-- <footer>

  <p>
    <small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small>
  </p>
</footer> -->
</div>
<script src="javascripts/scale.fix.js"></script>

<script>
    function news() {
      var x = document.getElementById("news");
      if (x.style.display === "none") {
        x.style.display = "block";
      } else {
        x.style.display = "none";
      }
    }
    function papers() {
      var x = document.getElementById("papers");
      if (x.style.display === "none") {
        x.style.display = "block";
      } else {
        x.style.display = "none";
      }
    }

    function blogs() {
      var x = document.getElementById("blogs");
      if (x.style.display === "none") {
        x.style.display = "block";
      } else {
        x.style.display = "none";
      }
    }

    function uses() {
      var x = document.getElementById("uses");
      if (x.style.display === "none") {
        x.style.display = "block";
      } else {
        x.style.display = "none";
      }
    }
    </script>

  </body>
</html>
